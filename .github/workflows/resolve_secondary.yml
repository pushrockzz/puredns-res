name: PureDNS Resolution Worker (Secondary)

on:
  workflow_dispatch:
    inputs:
      primary_run_id:
        description: 'The run ID of the primary workflow for artifact correlation.'
        required: true
        type: string
      primary_github_server_url:
        description: 'The server URL of the primary GitHub instance (e.g., https://github.com)'
        required: true
        type: string 
      primary_repo:
        description: 'The owner/repo of the primary workflow'
        required: true
        type: string
      primary_repo_owner:
        description: 'The owner of the primary repository that triggered this workflow.'
        required: true
        type: string
      primary_repo_name:
        description: 'The name of the primary repository that triggered this workflow.'
        required: true
        type: string  
      chunk_package_artifact_name:
        description: 'The name of the artifact package containing all chunks and resolvers.'
        required: true
        type: string
      matrix_json:
        description: 'The JSON string representing the matrix of chunks assigned to this worker.'
        required: true
        type: string

permissions:
  contents: read
  actions: read

jobs:
  process_assigned_chunks:
    name: Process Chunk (${{ matrix.pair.chunk }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      max-parallel: 20
      matrix:
        pair: ${{ fromJson(github.event.inputs.matrix_json || '[]') }}
    steps:
      - name: Display Trigger Payload (Debug)
        run: |
          echo "SECONDARY WORKER: Received payload:"
          echo "Primary Repo: ${{ github.event.inputs.primary_repo }}"
          echo "Primary Run ID: ${{ github.event.inputs.primary_run_id }}"
          echo "Artifact Name: ${{ github.event.inputs.chunk_package_artifact_name }}"
          echo "---"
          echo "Assigned Chunk for this job: ${{ toJson(matrix.pair) }}"

      - name: Checkout repository
        uses: actions/checkout@v4
        with: 
            fetch-depth: 0

      - name: Setup Go
        uses: actions/setup-go@v3
        with:
          go-version: '1.23'            # or whichever version you need

      - name: Cache Go modules and build cache
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-mod-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-mod-
            

      - name: Download Chunks Package from Primary Workflow
        env:
          GH_TOKEN: ${{ secrets.PAT_FOR_PRIMARY_ARTIFACTS_READ }}
          PRIMARY_REPO: ${{ github.event.inputs.primary_repo }}
          PRIMARY_RUN_ID: ${{ github.event.inputs.primary_run_id }}
          ARTIFACT_NAME: ${{ github.event.inputs.chunk_package_artifact_name }}          
        shell: bash
        run: |

          echo "WORKER: Downloading artifact '${{ github.event.inputs.chunk_package_artifact_name }}'..."
          
          # --- MODIFICATION: Added 'sudo' to all system commands ---
          echo "Installing GitHub CLI with root privileges..."
          sudo apt-get update -qy
          sudo apt-get install -qy curl
          
          # Use a pipe to sudo tee for writing the keyring to a protected directory
          curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo tee /usr/share/keyrings/githubcli-archive-keyring.gpg > /dev/null
          
          # Use a pipe to sudo tee for writing the source list
          echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null
          
          sudo apt-get update -qy
          sudo apt-get install -qy gh
          # --- END MODIFICATION ---

          # Download artifact (this command does not need sudo)
          gh run download ${{ github.event.inputs.primary_run_id }} -R ${{ github.event.inputs.primary_repo }} -n ${{ github.event.inputs.chunk_package_artifact_name }} --dir .
          
          if [ ! -d "chunks" ]; then
            echo "::error:: Artifact download failed or did not contain the 'chunks' directory."
            exit 0
          fi
          echo "Artifact downloaded successfully."
          
      - name: Ensure Tools are installed
        shell: bash
        run: |
          if ! command -v dsieve &> /dev/null; then
            echo "ðŸ› ï¸  dsieve not found; installing..."
            GO111MODULE=on go install \
             github.com/trickest/dsieve@latest
            echo "âœ… Installed dsieve to $(go env GOPATH)/bin/dsieve"
          else
            echo "âœ… dsieve already installed at $(command -v dsieve)"
          fi
         
          # Installing cut-cdn
          if ! command -v cut-cdn >/dev/null; then
            echo "Installing cut-cdnâ€¦"
            go install github.com/ImAyrix/cut-cdn@latest
          else
            echo "cut-cdn already in cache"
          fi       
          
          # Installing smap
          if ! command -v smap >/dev/null; then
            echo "Installing smapâ€¦"
            go install -v github.com/s0md3v/smap/cmd/smap@latest
          else
            echo "smap already in cache"
          fi   
          # Installing anew
          if ! command -v anew >/dev/null; then
            echo "Installing anewâ€¦"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi  
          
          # Installing naabu Prerequisites
          sudo apt install -y libpcap-dev

          # Installing naabu
          if ! command -v naabu >/dev/null; then
            echo "Installing naabuâ€¦"
            go install -v github.com/projectdiscovery/naabu/v2/cmd/naabu@latest
          else
            echo "naabu already in cache"
          fi      
          
      
      - name: Install preâ€‘reqs
        run: | 
          sudo apt-get update && sudo apt-get install -y jq curl

      - name: Install sanicdns
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # make sure the script is executable
          chmod +x scripts/install-sanicdns.sh
          # run the installer
          scripts/install-sanicdns.sh
     
      - name: Install ethtool & shrink NIC queues
        run: |
          sudo apt-get update -qq
          for i in {1..3}; do
            sudo DEBIAN_FRONTEND=noninteractive apt-get install -y -qq jq ethtool && break
            echo "apt install failed, retrying in 10sâ€¦" >&2
            sleep 10
          done
      
      - name: Resize NIC RSS queues
        run: |
          IFACE=$(ip route show default 2>/dev/null | awk '/default/ {print $5}' | head -n1)
          if [ -n "$IFACE" ]; then
            echo "ðŸ”§ Resizing RSS queues on $IFACE to 3..."
            sudo ethtool -L "$IFACE" combined 3 || true
          else
            echo "âš ï¸  No default interface detected; skipping RSS resize."
          fi
      
      - name: Allocate hugepages (1 GB)
        run: |
           # best-effort; if dpdk-hugepages.py isn't on PATH this will fail harmlessly
           sudo dpdk-hugepages.py --setup 2G || true

           
      - name: Resolve, Filter, Scan, and Sort
        id: resolve_and_sort
        shell: bash
        env:
          TERM: xterm        
        run: |
          
          CHUNK_FILE="${{ matrix.pair.chunk }}"
          JOB_OUTPUT_DIR="job_output"
          mkdir -p "$JOB_OUTPUT_DIR"

          echo "Processing chunk: '$CHUNK_FILE'..."
          if [ ! -s "$CHUNK_FILE" ]; then
            echo "Chunk file is empty. Exiting."
            exit 0
          fi

          echo "Installing resolvers.txt"
          wget -qO resolvers.txt https://raw.githubusercontent.com/and0x00/resolvers.txt/refs/heads/main/resolvers.txt
          
          # 1. Resolve with sanicdns
          SANICDNS_OUT="$JOB_OUTPUT_DIR/sanicdns_output.txt"
          LOG_FILE="$JOB_OUTPUT_DIR/wildcard_$(basename $CHUNK_FILE).log"

          echo "Resolving chunk '$CHUNK_FILE'..."
          sudo sanicdns \
            -i "$CHUNK_FILE" \
            -r 1000 \
            -c 3000 \
            -l "$LOG_FILE" \
            --resolvers resolvers.txt \
            -o "$SANICDNS_OUT"  \
            -w 4 \
            --num-retries 7 \
            --timeout 7000
            
          # Define output files
          FILTER_OUT="$JOB_OUTPUT_DIR/sanicdns_filter.txt"
          IP_LIST="$JOB_OUTPUT_DIR/ip_list.txt"
          NON_CDN_IP="$JOB_OUTPUT_DIR/non_cdn_ip.txt"
          CDN_IP="$JOB_OUTPUT_DIR/cdn_ip.txt"
          NAABU_OUT="$JOB_OUTPUT_DIR/naabu.txt"
          SUBDOMAIN_PORTS="$JOB_OUTPUT_DIR/subdomains_ports.txt"

          # 2. Filter ALL resolved subdomains (A, AAAA, CNAME) and extract unique IPs
          echo "-> Step: Extracting all resolved subdomains (A, AAAA, CNAME)..."
          jq -r 'select(.data.answers | length > 0) | .name | rtrimstr(".")' "$SANICDNS_OUT" | anew -q "$FILTER_OUT"
          echo "-> Found $(wc -l < "$FILTER_OUT") total resolved subdomains."
          
          # Extract only IPs from A and AAAA records (CNAME doesn't have IPs directly)
          echo "-> Step: Extracting unique IPs from A and AAAA records..."
          jq -r '.data.answers[]? | select(.type == "A" or .type == "AAAA") | .data' "$SANICDNS_OUT" | sort -u > "$IP_LIST"
          echo "-> Found $(wc -l < "$IP_LIST") total unique IPs."

          # 3. Separate CDN and Non-CDN IPs
          echo "-> Step: Using cut-cdn to filter for Non-CDN IPs..."
          cat "$IP_LIST" | cut-cdn -ua -silent -o "$NON_CDN_IP"
          echo "-> Found $(wc -l < "$NON_CDN_IP") Non-CDN IPs."

          echo "-> Step: Comparing non-cdn with IP list to aggregate CDN IPs..."
          grep -Fxv -f "$NON_CDN_IP" "$IP_LIST" > "$CDN_IP" || true
          echo "-> Found $(wc -l < "$CDN_IP") CDN IPs."

          # 4. Prepare subdomain_ports.txt file
          touch "$SUBDOMAIN_PORTS"
          chmod +x map_IP_to_HOST.py

          # 5. Map CDN IPs directly to subdomains (no port scanning for CDN)
          # This will include subdomains with CNAME -> CDN IP
          if [ -s "$CDN_IP" ]; then
            echo "-> Step: Mapping CDN IPs to subdomains (includes CNAME chains, ports 80,443 implied)..."
            ./map_IP_to_HOST.py "$SANICDNS_OUT" "$CDN_IP" | anew -q "$SUBDOMAIN_PORTS"
            echo "-> Mapped $(wc -l < "$CDN_IP") CDN IPs to subdomains."
          fi

          # 6. Map Non-CDN IPs directly to subdomains (ports 80,443 implied)
          # This will include subdomains with CNAME -> Non-CDN IP
          if [ -s "$NON_CDN_IP" ]; then
            echo "-> Step: Mapping Non-CDN IPs to subdomains (includes CNAME chains, ports 80,443 implied)..."
            ./map_IP_to_HOST.py "$SANICDNS_OUT" "$NON_CDN_IP" | anew -q "$SUBDOMAIN_PORTS"
            echo "-> Mapped $(wc -l < "$NON_CDN_IP") Non-CDN IPs to subdomains."
          fi

          echo "-> Current entries in subdomains_ports.txt: $(wc -l < "$SUBDOMAIN_PORTS")"

          # 7. Run naabu on Non-CDN IPs to find additional ports (excluding 80,443)
          if [ -s "$NON_CDN_IP" ]; then
            echo "-> Step: Running naabu in passive mode on Non-CDN IPs (excluding ports 80,443)..."
            naabu -l "$NON_CDN_IP" \
              -passive \
              -exclude-ports 80,443 \
              -silent \
              -o "$NAABU_OUT" || true
            
            if [ -s "$NAABU_OUT" ]; then
              echo "-> Naabu found $(wc -l < "$NAABU_OUT") IP:PORT combinations."
              echo "-> Step: Mapping naabu results (IP:PORT) to subdomains (includes CNAME chains)..."
              ./map_IP_to_HOST.py "$SANICDNS_OUT" "$NAABU_OUT" | anew -q "$SUBDOMAIN_PORTS"
            else
              echo "-> Naabu found no additional open ports (excluding 80,443)."
            fi
          else
            echo "-> No Non-CDN IPs found. Skipping naabu scan."
          fi

          echo "-> Final mapping complete. Total entries in subdomains_ports.txt: $(wc -l < "$SUBDOMAIN_PORTS")"

      - name: Sort Results into Root Domain Folders
        shell: bash
        run: |
          FINAL_SORTED_DIR="final_sorted"
          FILTER_OUT="job_output/sanicdns_filter.txt"
          SUBDOMAIN_PORTS="job_output/subdomains_ports.txt"
          mkdir -p "$FINAL_SORTED_DIR"
          
          if [ ! -s "$FILTER_OUT" ] && [ ! -s "$SUBDOMAIN_PORTS" ]; then
            echo "No resolved domains or port data to sort."
            exit 0
          fi
          
          while read -r root_domain; do
            if [ -z "$root_domain" ]; then continue; fi
            mkdir -p "$FINAL_SORTED_DIR/$root_domain"
            # Sort the enriched subdomain:port results
            if [ -s "$SUBDOMAIN_PORTS" ]; then
              grep -E "(^|\\.)${root_domain//./\\.}(\$|:)" "$SUBDOMAIN_PORTS" | anew -q "$FINAL_SORTED_DIR/$root_domain/subdomains_ports.txt" || true
            fi
            # Sort the simple resolved subdomains
            if [ -s "$FILTER_OUT" ]; then
              grep -E "(^|\\.)${root_domain//./\\.}(\$)" "$FILTER_OUT" | anew -q "$FINAL_SORTED_DIR/$root_domain/resolved_subdomains.txt" || true
            fi
          done < root_domains.txt

      - name: Upload Sorted Job Results
        uses: actions/upload-artifact@v4
        with:
          name: job-results-secondary-${{ github.event.inputs.primary_run_id }}-${{ strategy.job-index }}
          path: final_sorted/
          retention-days: 1

# This is a new job that only merges artifacts
  merge_results:
    name: Merge All Distributed Results
    needs: process_assigned_chunks
    runs-on: ubuntu-latest
    if: always() # Run even if some primary chunks fail, to merge what's available
    outputs:
      has_results: ${{ steps.deduplicate.outputs.has_results }}
    steps:

      - name: Download All Primary Job Results
        uses: actions/download-artifact@v4
        with:
          pattern: "job-results-secondary-*"
          path: temp-aggregated-results


      - name: De-duplicate and Verify Results
        id: deduplicate
        shell: bash
        run: |
          TEMP_DIR="temp-aggregated-results"
          FINAL_RESULTS_DIR="final-results"
          
          # Check if download produced any files
          if [ ! -d "$TEMP_DIR" ] || [ -z "$(ls -A "$TEMP_DIR" 2>/dev/null)" ]; then
            echo "::warning::No result artifacts were downloaded or they were empty."
            echo "has_results=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          
          echo "Downloaded artifacts structure:"
          ls -R "$TEMP_DIR"
          
          # Create final results directory
          mkdir -p "$FINAL_RESULTS_DIR"
          
          echo "Merging and de-duplicating results from all artifacts..."
          
          # Find all unique domain names across all artifacts
          # Structure: temp-aggregated-results/job-results-primary-*/DOMAIN_NAME/
          declare -A domains
          
          for artifact_dir in "$TEMP_DIR"/job-results-primary-*; do
            if [ ! -d "$artifact_dir" ]; then continue; fi
            
            # Look INSIDE each artifact directory for domain directories
            for domain_dir in "$artifact_dir"/*; do
              if [ ! -d "$domain_dir" ]; then continue; fi
              
              domain_name=$(basename "$domain_dir")
              domains["$domain_name"]=1
              
              echo "  Found domain: $domain_name in $(basename "$artifact_dir")"
            done
          done
          
          if [ ${#domains[@]} -eq 0 ]; then
            echo "::warning::No domain directories found in artifacts."
            echo "has_results=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          
          echo ""
          echo "Found ${#domains[@]} unique domains to merge: ${!domains[@]}"
          echo ""
          
          # Process each unique domain
          for domain_name in "${!domains[@]}"; do
            dest_dir="$FINAL_RESULTS_DIR/$domain_name"
            mkdir -p "$dest_dir"
            
            echo "Processing domain: $domain_name"
            
            # Merge resolved_subdomains.txt from ALL artifacts for this domain
            for artifact_dir in "$TEMP_DIR"/job-results-primary-*; do
              source_file="$artifact_dir/$domain_name/resolved_subdomains.txt"
              if [ -f "$source_file" ]; then
                echo "  - Merging resolved_subdomains.txt from $(basename "$artifact_dir")"
                cat "$source_file" >> "$dest_dir/resolved_subdomains.txt.tmp"
              fi
            done
            
            # Merge subdomains_ports.txt from ALL artifacts for this domain
            for artifact_dir in "$TEMP_DIR"/job-results-primary-*; do
              source_file="$artifact_dir/$domain_name/subdomains_ports.txt"
              if [ -f "$source_file" ]; then
                echo "  - Merging subdomains_ports.txt from $(basename "$artifact_dir")"
                cat "$source_file" >> "$dest_dir/subdomains_ports.txt.tmp"
              fi
            done
          done
          
          echo ""
          echo "De-duplicating all merged result files..."
          
          find "$FINAL_RESULTS_DIR" -type f -name "*.tmp" | while read -r tmp_file; do
            final_file="${tmp_file%.tmp}"
            
            if [ ! -s "$tmp_file" ]; then
              echo "  - Skipping empty file: $tmp_file"
              rm "$tmp_file"
              continue
            fi
            
            sort -u "$tmp_file" > "$final_file"
            rm "$tmp_file"
            
            entries=$(wc -l < "$final_file")
            domain=$(basename $(dirname "$final_file"))
            filename=$(basename "$final_file")
            echo "  - $domain/$filename: $entries unique entries"
          done
          
          # Remove empty directories
          find "$FINAL_RESULTS_DIR" -type d -empty -delete
          
          # Final verification
          if [ -z "$(ls -A "$FINAL_RESULTS_DIR" 2>/dev/null)" ]; then
            echo "::warning::No results after merging and de-duplication."
            echo "has_results=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          
          echo "has_results=true" >> "$GITHUB_OUTPUT"
          echo ""
          echo "=== Final Results Structure ==="
          ls -R "$FINAL_RESULTS_DIR"
          
          # Summary
          echo ""
          echo "=== Merge Summary ==="
          total_domains=0
          total_resolved=0
          total_ports=0
          
          for domain_dir in "$FINAL_RESULTS_DIR"/*; do
            if [ ! -d "$domain_dir" ]; then continue; fi
            
            domain=$(basename "$domain_dir")
            resolved_count=0
            ports_count=0
            
            if [ -f "$domain_dir/resolved_subdomains.txt" ]; then
              resolved_count=$(wc -l < "$domain_dir/resolved_subdomains.txt")
            fi
            
            if [ -f "$domain_dir/subdomains_ports.txt" ]; then
              ports_count=$(wc -l < "$domain_dir/subdomains_ports.txt")
            fi
            
            echo "  $domain: $resolved_count resolved, $ports_count with ports"
            total_domains=$((total_domains + 1))
            total_resolved=$((total_resolved + resolved_count))
            total_ports=$((total_ports + ports_count))
          done
          
          echo ""
          echo "Total: $total_domains domains, $total_resolved resolved subdomains, $total_ports with ports"

      - name: Upload Final Consolidated Results
        if: steps.deduplicate.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: puredns-final-sorted-results
          path: final-results/
          retention-days: 1    

  # This is the new, separate job for committing
  commit_results:
    name: Commit Results to store-recon
    needs: merge_results
    runs-on: ubuntu-latest
    if: always() && needs.merge_results.outputs.has_results == 'true'
    steps:
      - name: Download Consolidated Results
        uses: actions/download-artifact@v4
        with:
          name: puredns-final-sorted-results
          path: final-results

      - name: Commit to 'store-recon' Repository
        env:
          STORE_RECON_PAT: ${{ secrets.STORE_RECON_PAT }}
          STORE_RECON_REPO: ${{ secrets.STORE_RECON_REPO }}
          # This logic safely gets the correlation ID regardless of trigger type
          CORRELATION_ID: ${{ github.event.client_payload.run_id || github.run_id }}
        run: |
          
          RESULTS_DIR="${GITHUB_WORKSPACE}/final-results"
          CLONE_DIR="${GITHUB_WORKSPACE}/store-recon-clone"

          echo "Cloning target repository for commit..."
          git config --global user.name "Primary Worker Bot"
          git config --global user.email "bot@github.actions"
          git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${STORE_RECON_REPO}.git" "$CLONE_DIR"
          cd "$CLONE_DIR"

          run_merge() {
            for domain_dir in "${RESULTS_DIR}"/*; do
              if [ ! -d "$domain_dir" ]; then continue; fi
              domain_name=$(basename "$domain_dir")
              dest_repo_dir="results/$domain_name"
              mkdir -p "$dest_repo_dir"
              
              src_resolved="$domain_dir/resolved_subdomains.txt"
              dest_puredns="$dest_repo_dir/puredns_results.txt"
              if [ -s "$src_resolved" ]; then
                temp_merged=$(mktemp)
                if [ -f "$dest_puredns" ]; then cat "$src_resolved" "$dest_puredns" | sort -u > "$temp_merged"; else sort -u "$src_resolved" > "$temp_merged"; fi
                mv "$temp_merged" "$dest_puredns"
              fi
              
              src_ports="$domain_dir/subdomains_ports.txt"
              dest_ports="$dest_repo_dir/subdomains_ports.txt"
              if [ -s "$src_ports" ]; then
                temp_merged=$(mktemp)
                if [ -f "$dest_ports" ]; then cat "$src_ports" "$dest_ports" | sort -u > "$temp_merged"; else sort -u "$src_ports" > "$temp_merged"; fi
                mv "$temp_merged" "$dest_ports"
              fi
            done
            git add results/
          }

          run_merge
          if git diff --cached --quiet; then
            echo "No new unique data to commit."
            exit 0
          fi
          
          # Using the exact commit message pattern you requested
          git commit -m "feat: Add resolved subdomains from distributed scan Correlation ID: ${CORRELATION_ID}"
          
          MAX_ATTEMPTS=5
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
            echo "[Attempt $i/$MAX_ATTEMPTS] Pushing changes..."
            if git push origin main; then
              echo "âœ… Successfully pushed results to store-recon."
              exit 0
            fi
            echo "::warning:: Push failed. Retrying after pull/rebase..."
            git fetch origin main
            git reset --hard origin/main
            run_merge
            if git diff --cached --quiet; then
              echo "No net new changes after syncing. Another worker likely pushed this data."
              exit 0
            fi
            git commit -m "feat: Add resolved subdomains from distributed scan Correlation ID: ${CORRELATION_ID}"
            sleep $(( 5 * i ))
          done       
          
          echo "::error:: All push attempts failed."
          exit 0
