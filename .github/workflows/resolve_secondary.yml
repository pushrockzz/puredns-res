name: PureDNS Resolution Worker (Secondary)

on:
  workflow_dispatch:
    inputs:
      primary_run_id:
        description: 'The run ID of the primary workflow for artifact correlation.'
        required: true
        type: string
      primary_github_server_url:
        description: 'The server URL of the primary GitHub instance (e.g., https://github.com)'
        required: true
        type: string 
      primary_repo:
        description: 'The owner/repo of the primary workflow'
        required: true
        type: string
      primary_repo_owner:
        description: 'The owner of the primary repository that triggered this workflow.'
        required: true
        type: string
      primary_repo_name:
        description: 'The name of the primary repository that triggered this workflow.'
        required: true
        type: string  
      chunk_package_artifact_name:
        description: 'The name of the artifact package containing all chunks and resolvers.'
        required: true
        type: string
      matrix_json:
        description: 'The JSON string representing the matrix of chunks assigned to this worker.'
        required: true
        type: string

permissions:
  contents: read
  actions: read

jobs:
  process_assigned_chunks:
    name: Process Chunk (${{ matrix.pair.chunk }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      max-parallel: 20
      matrix:
        pair: ${{ fromJson(github.event.inputs.matrix_json || '[]') }}
    steps:
      - name: Display Trigger Payload (Debug)
        run: |
          echo "SECONDARY WORKER: Received payload:"
          echo "Primary Repo: ${{ github.event.inputs.primary_repo }}"
          echo "Primary Run ID: ${{ github.event.inputs.primary_run_id }}"
          echo "Artifact Name: ${{ github.event.inputs.chunk_package_artifact_name }}"
          echo "---"
          echo "Assigned Chunk for this job: ${{ toJson(matrix.pair) }}"

      - name: Checkout repository
        uses: actions/checkout@v4
        with: 
            fetch-depth: 0

      - name: Setup Go
        uses: actions/setup-go@v3
        with:
          go-version: '1.23'            # or whichever version you need

      - name: Cache Go modules and build cache
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-mod-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-mod-
            

      - name: Download Chunks Package from Primary Workflow
        env:
          GH_TOKEN: ${{ secrets.PAT_FOR_PRIMARY_ARTIFACTS_READ }}
          PRIMARY_REPO: ${{ github.event.inputs.primary_repo }}
          PRIMARY_RUN_ID: ${{ github.event.inputs.primary_run_id }}
          ARTIFACT_NAME: ${{ github.event.inputs.chunk_package_artifact_name }}          
        shell: bash
        run: |

          echo "WORKER: Downloading artifact '${{ github.event.inputs.chunk_package_artifact_name }}'..."
          
          # --- MODIFICATION: Added 'sudo' to all system commands ---
          echo "Installing GitHub CLI with root privileges..."
          sudo apt-get update -qy
          sudo apt-get install -qy curl
          
          # Use a pipe to sudo tee for writing the keyring to a protected directory
          curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo tee /usr/share/keyrings/githubcli-archive-keyring.gpg > /dev/null
          
          # Use a pipe to sudo tee for writing the source list
          echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null
          
          sudo apt-get update -qy
          sudo apt-get install -qy gh
          # --- END MODIFICATION ---

          # Download artifact (this command does not need sudo)
          gh run download ${{ github.event.inputs.primary_run_id }} -R ${{ github.event.inputs.primary_repo }} -n ${{ github.event.inputs.chunk_package_artifact_name }} --dir .
          
          if [ ! -d "chunks" ]; then
            echo "::error:: Artifact download failed or did not contain the 'chunks' directory."
            exit 0
          fi
          echo "Artifact downloaded successfully."
          
      - name: Ensure Tools are installed
        shell: bash
        run: |
          if ! command -v dsieve &> /dev/null; then
            echo "🛠️  dsieve not found; installing..."
            GO111MODULE=on go install \
             github.com/trickest/dsieve@latest
            echo "✅ Installed dsieve to $(go env GOPATH)/bin/dsieve"
          else
            echo "✅ dsieve already installed at $(command -v dsieve)"
          fi
         
          # Installing cut-cdn
          if ! command -v cut-cdn >/dev/null; then
            echo "Installing cut-cdn…"
            go install github.com/ImAyrix/cut-cdn@latest
          else
            echo "cut-cdn already in cache"
          fi       
          
          # Installing smap
          if ! command -v smap >/dev/null; then
            echo "Installing smap…"
            go install -v github.com/s0md3v/smap/cmd/smap@latest
          else
            echo "smap already in cache"
          fi   
          # Installing anew
          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi    
      
      - name: Install pre‑reqs
        run: | 
          sudo apt-get update && sudo apt-get install -y jq curl

      - name: Install sanicdns
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # make sure the script is executable
          chmod +x scripts/install-sanicdns.sh
          # run the installer
          scripts/install-sanicdns.sh
     
      - name: Install ethtool & shrink NIC queues
        run: |
          sudo apt-get update -qq
          for i in {1..3}; do
            sudo DEBIAN_FRONTEND=noninteractive apt-get install -y -qq jq ethtool && break
            echo "apt install failed, retrying in 10s…" >&2
            sleep 10
          done
      
      - name: Resize NIC RSS queues
        run: |
          IFACE=$(ip route show default 2>/dev/null | awk '/default/ {print $5}' | head -n1)
          if [ -n "$IFACE" ]; then
            echo "🔧 Resizing RSS queues on $IFACE to 3..."
            sudo ethtool -L "$IFACE" combined 3 || true
          else
            echo "⚠️  No default interface detected; skipping RSS resize."
          fi
      
      - name: Allocate hugepages (1 GB)
        run: |
           # best-effort; if dpdk-hugepages.py isn't on PATH this will fail harmlessly
           sudo dpdk-hugepages.py --setup 2G || true

      - name: Resolve, Filter, Scan, and Sort
        id: resolve_and_sort
        shell: bash
        env:
          TERM: xterm        
        run: |
          # This entire block is a direct copy of the processing logic from the Primary workflow
          # to ensure 100% consistency in results.
          
          CHUNK_FILE="${{ matrix.pair.chunk }}"
          JOB_OUTPUT_DIR="job_output"
          mkdir -p "$JOB_OUTPUT_DIR"

          echo "Processing chunk: '$CHUNK_FILE'..."
          if [ ! -s "$CHUNK_FILE" ]; then
            echo "Chunk file is empty. Exiting."
            exit 0
          fi
          
          echo "Installing resolvers.txt"

          wget -qO resolvers.txt https://raw.githubusercontent.com/and0x00/resolvers.txt/refs/heads/main/resolvers.txt          
          
          # 1. Resolve with sanicdns (and required setup)

          SANICDNS_OUT="$JOB_OUTPUT_DIR/sanicdns_output.txt"
          sudo sanicdns \
            -i "$CHUNK_FILE" -r 1000 -c 3000 \
            --resolvers resolvers.txt \
            -o "$SANICDNS_OUT" -w 4 --num-retries 7 --timeout 7000

          # 2. Filter and Process IPs
          FILTER_OUT="$JOB_OUTPUT_DIR/sanicdns_filter.txt"
          IP_LIST="$JOB_OUTPUT_DIR/ip_list.txt"
          NON_CDN_IP="$JOB_OUTPUT_DIR/non_cdn_ip.txt"
          SMAP_OUT="$JOB_OUTPUT_DIR/smap.txt"
          SUBDOMAIN_PORTS="$JOB_OUTPUT_DIR/subdomains_ports.txt"
          
          jq -r 'select(.data.answers | any(.type=="A")) | .name | rtrimstr(".")' "$SANICDNS_OUT" | anew -q "$FILTER_OUT"
          jq -r 'select(.data.answers | any(.type=="A")) | .data.answers[] | select(.type == "A") | .data' "$SANICDNS_OUT" | sort -u > "$IP_LIST"
          
          cat "$IP_LIST" | cut-cdn -ua -silent -o "$NON_CDN_IP"
          grep -Fxv -f "$NON_CDN_IP" "$IP_LIST" > "$JOB_OUTPUT_DIR/cdn_ip.txt" || true
          
          if [ -s "$NON_CDN_IP" ]; then smap -iL "$NON_CDN_IP" -oP "$SMAP_OUT"; else touch "$SMAP_OUT"; fi
          
          touch "$SUBDOMAIN_PORTS"
          chmod +x map_IP_to_HOST.py
          if [ -s "$SMAP_OUT" ]; then ./map_IP_to_HOST.py "$SANICDNS_OUT" "$SMAP_OUT" | anew -q "$SUBDOMAIN_PORTS"; fi
          if [ -s "$JOB_OUTPUT_DIR/cdn_ip.txt" ]; then ./map_IP_to_HOST.py "$SANICDNS_OUT" "$JOB_OUTPUT_DIR/cdn_ip.txt" | anew -q "$SUBDOMAIN_PORTS"; fi

          # 3. Sort results into root domain folders
          FINAL_SORTED_DIR="final_sorted"
          mkdir -p "$FINAL_SORTED_DIR"
          
          while read -r root_domain; do
            if [ -z "$root_domain" ]; then continue; fi
            mkdir -p "$FINAL_SORTED_DIR/$root_domain"
            grep -E "(^|\\.)${root_domain//./\\.}(\$|:)" "$SUBDOMAIN_PORTS" | anew -q "$FINAL_SORTED_DIR/$root_domain/subdomains_ports.txt" || true
            grep -E "(^|\\.)${root_domain//./\\.}(\$)" "$FILTER_OUT" | anew -q "$FINAL_SORTED_DIR/$root_domain/resolved_subdomains.txt" || true
          done < root_domains.txt

      - name: Upload Sorted Job Results
        uses: actions/upload-artifact@v4
        with:
          name: job-results-secondary-${{ github.event.inputs.primary_run_id }}-${{ strategy.job-index }}
          path: final_sorted/
          retention-days: 1

# This is a new job that only merges artifacts
  merge_results:
    name: Merge All Distributed Results
    needs: process_assigned_chunks
    runs-on: ubuntu-latest
    if: always()
    outputs:
      has_results: ${{ steps.consolidate.outputs.has_results }}
    steps:
      - name: Download All Job Results from All Workflows
        env:
          GH_TOKEN: ${{ secrets.PAT_FOR_PRIMARY_ARTIFACTS_READ }}
        shell: bash
        run: |
          
          # Install gh-cli if not present
          sudo apt-get update -qy && sudo apt-get install -qy curl && curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo tee /usr/share/keyrings/githubcli-archive-keyring.gpg > /dev/null && echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null && sudo apt-get update -qy && sudo apt-get install -qy gh
          
          echo "Downloading all artifacts from primary run ID: ${{ github.event.inputs.primary_run_id }}"
          gh run download ${{ github.event.inputs.primary_run_id }} \
            -R ${{ github.event.inputs.primary_repo }} \
            -p 'job-results-*' \
            --dir temp-aggregated-results
          
          if [ -z "$(ls -A temp-aggregated-results)" ]; then
            echo "::warning:: No result artifacts found from any worker. Nothing to merge."
            echo "HAS_RESULTS=false" >> $GITHUB_ENV
          else
            echo "HAS_RESULTS=true" >> $GITHUB_ENV
          fi

      - name: Consolidate and De-duplicate All Results
        id: consolidate
        if: env.HAS_RESULTS == 'true'
        shell: bash
        run: |
          FINAL_RESULTS_DIR="final-results"
          mkdir -p "$FINAL_RESULTS_DIR"
          echo "Aggregating results..."
          find temp-aggregated-results -mindepth 1 -type d -exec cp -r {}/. "$FINAL_RESULTS_DIR" \;
          echo "De-duplicating all result files..."
          find "$FINAL_RESULTS_DIR" -type f -name "*.txt" -exec sort -u -o {} {} \;
          echo "has_results=true" >> $GITHUB_OUTPUT
          ls -R "$FINAL_RESULTS_DIR"

      - name: Upload Consolidated Artifact for Commit Job
        if: steps.consolidate.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-results-${{ github.event.inputs.primary_run_id }}
          path: final-results/
          retention-days: 1

  # This is the new, separate job for committing
  commit_results:
    name: Commit Results to store-recon
    needs: merge_results
    runs-on: ubuntu-latest
    if: always() && needs.merge_results.outputs.has_results == 'true'
    steps:
      - name: Download Consolidated Results
        uses: actions/download-artifact@v4
        with:
          name: consolidated-results-${{ github.event.inputs.primary_run_id }}
          path: final-results

      - name: Commit to 'store-recon' Repository
        env:
          STORE_RECON_PAT: ${{ secrets.STORE_RECON_PAT_WRITE }}
          STORE_RECON_REPO: ${{ secrets.STORE_RECON_REPO }}
          # The Correlation ID is taken directly from the trigger input
          CORRELATION_ID: ${{ github.event.inputs.primary_run_id }}
        run: |
          
          RESULTS_DIR="${GITHUB_WORKSPACE}/final-results"
          CLONE_DIR="${GITHUB_WORKSPACE}/store-recon-clone"

          echo "Cloning target repository for commit..."
          git config --global user.name "Tertiary Worker Bot"
          git config --global user.email "bot@github.actions"
          git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${STORE_RECON_REPO}.git" "$CLONE_DIR"
          cd "$CLONE_DIR"

          run_merge() {
            for domain_dir in "${RESULTS_DIR}"/*; do
              if [ ! -d "$domain_dir" ]; then continue; fi
              domain_name=$(basename "$domain_dir")
              dest_repo_dir="results/$domain_name"
              mkdir -p "$dest_repo_dir"
              
              src_resolved="$domain_dir/resolved_subdomains.txt"
              dest_puredns="$dest_repo_dir/puredns_results.txt"
              if [ -s "$src_resolved" ]; then
                temp_merged=$(mktemp)
                if [ -f "$dest_puredns" ]; then cat "$src_resolved" "$dest_puredns" | sort -u > "$temp_merged"; else sort -u "$src_resolved" > "$temp_merged"; fi
                mv "$temp_merged" "$dest_puredns"
              fi
              
              src_ports="$domain_dir/subdomains_ports.txt"
              dest_ports="$dest_repo_dir/subdomains_ports.txt"
              if [ -s "$src_ports" ]; then
                temp_merged=$(mktemp)
                if [ -f "$dest_ports" ]; then cat "$src_ports" "$dest_ports" | sort -u > "$temp_merged"; else sort -u "$src_ports" > "$temp_merged"; fi
                mv "$temp_merged" "$dest_ports"
              fi
            done
            git add results/
          }

          run_merge
          if git diff --cached --quiet; then
            echo "No new unique data to commit."
            exit 0
          fi
          
          # Using the exact commit message pattern you requested
          git commit -m "feat: Add resolved subdomains from distributed scan Correlation ID: ${CORRELATION_ID}"
          
          MAX_ATTEMPTS=5
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
            echo "[Attempt $i/$MAX_ATTEMPTS] Pushing changes..."
            if git push origin main; then
              echo "✅ Successfully pushed results to store-recon."
              exit 0
            fi
            echo "::warning:: Push failed. Retrying after pull/rebase..."
            git fetch origin main
            git reset --hard origin/main
            run_merge
            if git diff --cached --quiet; then
              echo "No net new changes after syncing. Another worker likely pushed this data."
              exit 0
            fi
            git commit -m "feat: Add resolved subdomains from distributed scan Correlation ID: ${CORRELATION_ID}"
            sleep $(( 5 * i ))
          done
          
          echo "::error:: All push attempts failed."
          exit 0
